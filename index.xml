<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Chao-Chun (Joe) Hsu</title>
    <link>https://chaochunhsu.github.io/</link>
    <description>Recent content in Home on Chao-Chun (Joe) Hsu</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://chaochunhsu.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cupper</title>
      <link>https://chaochunhsu.github.io/print-version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chaochunhsu.github.io/print-version/</guid>
      <description>&lt;p&gt;You don&amp;rsquo;t want to edit this file :-)&lt;/p&gt;</description>
    </item>
    <item>
      <title>My Journey Building a Scalable, Cached Embedding Server with TEI and Qdrant</title>
      <link>https://chaochunhsu.github.io/patterns/blogs/tei_qdrant_cache/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chaochunhsu.github.io/patterns/blogs/tei_qdrant_cache/</guid>
      <description>&lt;h2 id=&#34;my-attempt-at-building-a-multi-gpu-cache-embedding-server-with-tei-and-qdrant&#34;&gt;My Attempt at Building a Multi-GPU Cache Embedding Server with TEI and Qdrant&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Github repo: &lt;a href=&#34;https://github.com/joe32140/tei-qdrant-cache&#34;&gt;https://github.com/joe32140/tei-qdrant-cache&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h3 id=&#34;before-we-start&#34;&gt;Before We Start&lt;/h3&gt;&#xA;&lt;p&gt;Serving an LLM-based embedding model with replicates over multiple GPUs on the same machine might sound trivial today (2025), but after my extensive search, there&amp;rsquo;s actually no one-click setup. Originally, I thought vLLM could easily fulfill my needs, or that SGLang could be an alternative if vLLM didn&amp;rsquo;t work. However, both libraries turned out to be either very limited to specific models or did not support request routing across multi-GPUs. Luckily, I encountered &lt;a href=&#34;https://docs.vllm.ai/en/latest/deployment/nginx.html&#34;&gt;link&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/text-embeddings-inference/issues/87#issuecomment-1822970062&#34;&gt;link&lt;/a&gt;, leading me to an Nginx load balancer setup. It also turned out that Hugging Face has its own text-embeddings-inference library, which provides seamless support for serving embedding models with pre-built Docker images. In this post, my goal was to create a robust system for serving text embeddings using Hugging Face&amp;rsquo;s text-embeddings-inference (TEI) server, specifically targeting multi-GPU setups and incorporating a mechanism to handle repeated requests efficiently â€“ essentially, building a smart caching layer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Publication</title>
      <link>https://chaochunhsu.github.io/patterns/pattern/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chaochunhsu.github.io/patterns/pattern/</guid>
      <description>&lt;h1 id=&#34;conferences--workshops&#34;&gt;Conferences &amp;amp; Workshops&lt;/h1&gt;&#xA;&lt;h3 id=&#34;2024&#34;&gt;2024&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;&#34;&gt;CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Erin Bransom, Jenna Sparks, Bailey Kuehl, Chenhao Tan, David Wadden, Lucy Lu Wang, Aakanksha Naik., (&lt;em&gt;Findings of ACL 2024&lt;/em&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2023&#34;&gt;2023&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.03077&#34;&gt;Clinical Notes Reveal Physician Fatigue&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;,  Ziad Obermeyer, Chenhao Tan, (&lt;em&gt;under submission&lt;/em&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2021&#34;&gt;2021&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.06896&#34;&gt;Decision-Focused Summarization&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Chenhao Tan, &lt;em&gt;EMNLP 2021&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.00955&#34;&gt;Answer Generation for Retrieval-based Question Answering Systems&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Eric Lind, Luca Soldaini, Alessandro Moschitti, &lt;em&gt;Findings of ACL 2021 (short)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2020&#34;&gt;2020&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.03574.pdf&#34;&gt;Characterizing the Value of Information in Medical Notes&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Shantanu Karnwal, S. Mullainathan, Ziad Obermeyer and Chenhao Tan, &lt;em&gt;Findings of EMNLP 2020&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1912.01496&#34;&gt;Knowledge-Enriched Visual Storytelling&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, Zi-Yuan Chen&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, Chi-Yang Hsu, Chih-Chia Li, Tzu-Yuan Lin, Ting-Hao (Kenneth) Huang, Lun-Wei Ku (&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt; equal contribution), &lt;em&gt;The AAAI Conference on Artificial Intelligence (AAAI&#39;20)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2019&#34;&gt;2019&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1903.02230&#34;&gt;Dixit: Interactive Visual Storytelling via Term Manipulation&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, Yu-Hua Chen&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, Zi-Yuan Chen&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, Hsin-Yu Lin, Ting-Hao (Kenneth) Huang, Lun-Wei Ku (&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt; equal contribution), &lt;em&gt;In proceedings of The Web Conference 2019 Demonstration track (WWW&#39;19 Demo)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://workshop.colips.org/dstc7/papers/25.pdf&#34;&gt;Entropy-Enhanced Multimodal Attention Model for Scene-Aware Dialogue Generation&lt;/a&gt;, Kuan-Yen Lin, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Yun-Nung Chen, Lun-Wei Ku, &lt;em&gt;In proceedings of Dialog System Technology Challenges 7 (DSTC7)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2018&#34;&gt;2018&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/W18-3505&#34;&gt;SocialNLP 2018 EmotionX Challenge Overview: Recognizing Emotions in Dialogues&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Lun-Wei Ku, &lt;em&gt;In proceedings of Workshop on Natural Language Processing for Social Media (SocialNLP2018)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.11867&#34;&gt;Using Inter-Sentence Diverse Beam Search to Reduce Redundancy in Visual Storytelling&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Szu-Min Chen, Ming-Hsun Hsieh, Lun-Wei Ku, &lt;em&gt;In proceedings of Storytelling Workshop 2018&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1802.08379&#34;&gt;EmotionLines: An Emotion Corpus of Multi-Party Conversations&lt;/a&gt;, Sheng-Yeh Chen&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, Chuan-Chun Kuo, Ting-Hao (Kenneth) Huang, Lun-Wei Ku (&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt; equal contribution), &lt;em&gt;In proceedings of The International Conference on Language Resources and Evaluation (LREC2018)&lt;/em&gt;, [&lt;a href=&#34;http://doraemon.iis.sinica.edu.tw/emotionlines/index.html&#34;&gt;Dataset link&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Reproducibility Report of ModernBERT Models for Retrieval Tasks Using DPR</title>
      <link>https://chaochunhsu.github.io/patterns/blogs/modernbert_dpr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chaochunhsu.github.io/patterns/blogs/modernbert_dpr/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;Cross-posted here: &lt;a href=&#34;https://api.wandb.ai/links/joe32140/zqs87nz3&#34;&gt;https://api.wandb.ai/links/joe32140/zqs87nz3&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;h1 id=&#34;before-we-start&#34;&gt;Before We Start&lt;/h1&gt;&#xA;&lt;p&gt;As researchers from LightOn.AI and Answer.AI released the ModernBERT models (&lt;a href=&#34;https://huggingface.co/papers/2412.13663&#34;&gt;https://huggingface.co/papers/2412.13663&lt;/a&gt; ), which are BERT models for 2024, I am interested to see its performance on retrieval tasks as mentioned in their paper, specifically with DPR. However, they have not released the model checkpoints for all experiments. I decided to finetune ModernBERT on the MSMACRO dataset by myself based on the provided training scripts.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
