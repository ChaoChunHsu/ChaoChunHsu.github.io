<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Chao-Chun (Joe) Hsu</title>
    <link>https://chaochunhsu.github.io/</link>
    <description>Recent content in Home on Chao-Chun (Joe) Hsu</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://chaochunhsu.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cupper</title>
      <link>https://chaochunhsu.github.io/print-version/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chaochunhsu.github.io/print-version/</guid>
      <description>&lt;p&gt;You don&amp;rsquo;t want to edit this file :-)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Publication</title>
      <link>https://chaochunhsu.github.io/patterns/pattern/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chaochunhsu.github.io/patterns/pattern/</guid>
      <description>&lt;h1 id=&#34;conferences--workshops&#34;&gt;Conferences &amp;amp; Workshops&lt;/h1&gt;&#xA;&lt;h3 id=&#34;2024&#34;&gt;2024&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;&#34;&gt;CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Erin Bransom, Jenna Sparks, Bailey Kuehl, Chenhao Tan, David Wadden, Lucy Lu Wang, Aakanksha Naik., (&lt;em&gt;Findings of ACL 2024&lt;/em&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2023&#34;&gt;2023&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.03077&#34;&gt;Clinical Notes Reveal Physician Fatigue&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;,  Ziad Obermeyer, Chenhao Tan, (&lt;em&gt;under submission&lt;/em&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2021&#34;&gt;2021&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2109.06896&#34;&gt;Decision-Focused Summarization&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Chenhao Tan, &lt;em&gt;EMNLP 2021&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.00955&#34;&gt;Answer Generation for Retrieval-based Question Answering Systems&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Eric Lind, Luca Soldaini, Alessandro Moschitti, &lt;em&gt;Findings of ACL 2021 (short)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2020&#34;&gt;2020&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2010.03574.pdf&#34;&gt;Characterizing the Value of Information in Medical Notes&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Shantanu Karnwal, S. Mullainathan, Ziad Obermeyer and Chenhao Tan, &lt;em&gt;Findings of EMNLP 2020&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1912.01496&#34;&gt;Knowledge-Enriched Visual Storytelling&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, Zi-Yuan Chen&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, Chi-Yang Hsu, Chih-Chia Li, Tzu-Yuan Lin, Ting-Hao (Kenneth) Huang, Lun-Wei Ku (&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt; equal contribution), &lt;em&gt;The AAAI Conference on Artificial Intelligence (AAAI&#39;20)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2019&#34;&gt;2019&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1903.02230&#34;&gt;Dixit: Interactive Visual Storytelling via Term Manipulation&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, Yu-Hua Chen&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, Zi-Yuan Chen&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, Hsin-Yu Lin, Ting-Hao (Kenneth) Huang, Lun-Wei Ku (&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt; equal contribution), &lt;em&gt;In proceedings of The Web Conference 2019 Demonstration track (WWW&#39;19 Demo)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://workshop.colips.org/dstc7/papers/25.pdf&#34;&gt;Entropy-Enhanced Multimodal Attention Model for Scene-Aware Dialogue Generation&lt;/a&gt;, Kuan-Yen Lin, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Yun-Nung Chen, Lun-Wei Ku, &lt;em&gt;In proceedings of Dialog System Technology Challenges 7 (DSTC7)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;2018&#34;&gt;2018&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/W18-3505&#34;&gt;SocialNLP 2018 EmotionX Challenge Overview: Recognizing Emotions in Dialogues&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Lun-Wei Ku, &lt;em&gt;In proceedings of Workshop on Natural Language Processing for Social Media (SocialNLP2018)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.11867&#34;&gt;Using Inter-Sentence Diverse Beam Search to Reduce Redundancy in Visual Storytelling&lt;/a&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;, Szu-Min Chen, Ming-Hsun Hsieh, Lun-Wei Ku, &lt;em&gt;In proceedings of Storytelling Workshop 2018&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1802.08379&#34;&gt;EmotionLines: An Emotion Corpus of Multi-Party Conversations&lt;/a&gt;, Sheng-Yeh Chen&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, &lt;strong&gt;&lt;em&gt;Chao-Chun Hsu&lt;/em&gt;&lt;/strong&gt;&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt;, Chuan-Chun Kuo, Ting-Hao (Kenneth) Huang, Lun-Wei Ku (&lt;!-- raw HTML omitted --&gt;*&lt;!-- raw HTML omitted --&gt; equal contribution), &lt;em&gt;In proceedings of The International Conference on Language Resources and Evaluation (LREC2018)&lt;/em&gt;, [&lt;a href=&#34;http://doraemon.iis.sinica.edu.tw/emotionlines/index.html&#34;&gt;Dataset link&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Reproducibility Report of ModernBERT Models for Retrieval Tasks Using DPR</title>
      <link>https://chaochunhsu.github.io/patterns/blogs/modernbert_dpr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://chaochunhsu.github.io/patterns/blogs/modernbert_dpr/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;Cross-posted here: &lt;a href=&#34;https://api.wandb.ai/links/joe32140/zqs87nz3&#34;&gt;https://api.wandb.ai/links/joe32140/zqs87nz3&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;h1 id=&#34;before-we-started&#34;&gt;Before We Started&lt;/h1&gt;&#xA;&lt;p&gt;As researchers from Lightn.io and answer.ai released the ModernBERT models (&lt;a href=&#34;https://huggingface.co/papers/2412.13663)&#34;&gt;https://huggingface.co/papers/2412.13663)&lt;/a&gt;, which are BERT models for 2024, I am interested to see its performance on retrieval tasks as mentioned in their paper, specifically with DPR. However, they have not released the model checkpoints for all experiments. I decided to finetune ModernBERT on the MSMACRO dataset by myself based on the provided training scripts.&lt;/p&gt;&#xA;&lt;h1 id=&#34;experiments&#34;&gt;Experiments&lt;/h1&gt;&#xA;&lt;p&gt;I ran experiments with &lt;a href=&#34;https://github.com/AnswerDotAI/ModernBERT/tree/main/examples&#34;&gt;the official training scripts&lt;/a&gt;, modifying the mini_batch_size for CachedMultipleNegativesRankingLoss to accelerate the training. Following the hyperparameter suggestions, I chose a learning rate of 8e-5 for the base model and 1e-4 for the large model. The per_device_batch_size was set to 512, which is different from the batch size of 16 mentioned in the paper. By training both model sizes on an RTX4090 24GB GPU, it took 1 hour for the base model and 2 hours for the large model for one epoch. More training logs are shown in the panels below.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
